---
title: 概率论
date: 2021-12-17 14:00:00 +0800
categories: [Math, Probability and Statistical Analysis]
tags: [mathematics, probability, statistics, analysis]     # TAG names should always be lowercase
math: true
toc: true
---

> 本博客是基于中国科学技术大学的庄玮玮老师的 PPT 整理而成

# 事件与概率

## 随机现象和随机事件

- **随机现象**: 自然界中的客观现象, 当人们观测它时, 所得结果不能预先确定, 而仅仅是多种可能结果之一. 如 "明天是否下雨".
- **随机试验**: 一个实验称为随机试验, 若该试验满足以下条件:
	- 在相同条件下课重复进行;
	- 所有的试验结果是明确可知的, 结果至少有两个;
	- 每次试验恰好出现这些结果中的一个, 但在试验之前无法预知该结果.

**基本概念**

- **样本点** ($$w$$): 基本试验结果
- **样本空间** ($$\Omega$$): 试验 $$S$$ 所有的样本点 $$w$$ 构成的集合, 即 $$\Omega=\{w\vert w\text{是试验的样本点}\}$$.
	- 样本空间可以有限, 也可以无限 (可数和不可数)
- **随机事件**: 样本空间 $$\Omega$$ 的子集 $$A\subset\Omega$$ 是事件.
	- 如果 $$w\in A$$, 则称事件 $$A$$ 发生, 否则称 $$A$$ 不发生.
	- **基本事件**: 单个样本点构成的事件
	- **复杂事件**: 由一个或若干个基本事件组成
	- 特殊事件: 必然事件 $$\Omega$$, 不可能事件 $$\varnothing$$ 

事件的运算符号和集合的运算符号是相同的, 运算规律也一致, 集合的运算详见各个代数学教程, 这里不再赘述.

用 $$\lvert A\rvert$$ 表示事件 $$A$$ 的样本点个数, 用 $$\lvert\Omega\rvert$$ 表示样本空间 $$\Omega$$ 中的样本点个数.

## 古典概型
- **概率**: 设 $$\Omega$$ 是样本空间. 对于 $$\Omega$$ 的事件 $$A\subset\Omega$$, 我们用 $$[0,1]$$ 中的数 $$P(A)$$ 表示 $$A$$ 发生的可能性的大小, 称 $$P(A)$$ 是事件 $$A$$ 发生的概率, 简称为 $$A$$ 的概率.
	- 按照以上原则, 如果事件 $$A,B$$ 发生的可能性相同, 则有 $$P(A)=P(B)$$.
- **古典概型**: 试验结果有限, 等可能性
	- 一个试验 $$S$$ 有有限个试验结果 $$\{w_1, w_2, \cdots, w_n\}$$.
	- 找不到理由认为一个试验结果比另一个试验结果更易于发生.
- **古典概率**: 设试验 $$S$$ 的样本空间 $$\Omega$$ 是有限集合, $$A\subset\Omega$$. 如果 $$\Omega$$ 的每个样本点发生的可能性相同, 则称
  $$
  P(A)=\frac{\lvert A\rvert}{\lvert\Omega\rvert}
 $$
 为试验 $$S$$ 下 $$A$$ 发生的概率, 简称为事件 $$A$$ 的概率.

### 一些计数模式
- 排列组合
	- 从 $$n$$ 个不同的元素中有放回地每次抽取一个, 依次抽取 $$m$$ 个排成一列, 可以得到 $$n^m$$ 个不同的排列. 当随机抽取时, 得到的不同排列是等可能的, $$p=\frac1{n^m}$$.
	- 从 $$n$$ 个不同的元素中无放回地抽取 $$m$$ 个元素排成一列时, 可以得到 $$\mathrm A^m_n=\frac{n!}{(n-m)!}$$ 个不同的排列. 当随机抽取和排列时, 得到的不同排列是等可能的.
	- 从 $$n$$ 个不同的元素中无放回地抽取 $$m$$ 个元素, 不论次序地组成一组,  可以得到 $$\mathrm C^m_n=\frac{n!}{m!(n-m)!}$$ 个不同的组合. 当随机抽取时, 得到的不同组合是等可能的.
	- 将 $$n$$ 个不同的元素分成有次序的 $$k$$ 组, 不考虑每组中元素的次序, 第 $$i(1\le i\le k)$$ 组恰好有 $$n_i$$ 个元素的不同结果数是 $$\frac{n!}{\prod^k_{i=1}n_i!}$$, 当随机分组时, 得到的不同结果是等可能的.
- 不相邻问题: 现有 $$A,B$$ 两类不同的个体做全排列, 使得 $$B$$ 类中个体在排列中互不相邻, 求排列数.
	- 先将 $$A$$ 类个体做全排列, 然后在 $$A$$ 类排列的每相邻两个个体之间插入一个空位, 在 $$A$$ 类个体的最左侧和最右侧各插入一个空位, 再在这些空位中选取适当个数的空位来排列  $$B$$ 类个体.

## 概率的公理化
设 $$\Omega$$ 是试验 $$S$$ 的样本空间, 事件必须是 $$\Omega$$ 的子集, 且满足以下条件:
- $$\Omega$$ 和空集 $$\varnothing$$ 是事件
- 事件经过有限次集合运算得到的集合是事件
- 如果 $$A_j$$ 是事件, 则 $$\bigcup_{j=1}^\infty A_j$$ 是事件

对于事件 $$A$$, 概率 $$P(A)$$ 是表示 $$A$$ 发生的可能性的大小的实数, 必须满足以下三个条件 (概率的**公理化条件**):
- **非负性**: $$\forall\text{事件}A, P(A)\ge0$$
- **完全性**: $$P(\Omega)=1, P(\varnothing)=0$$
- **可加性**: 对于互不相容的事件 $$A_1, A_2, \cdots$$, 有
$$
P\left(\bigcup_{j=1}^n A_j\right)=\sum_{j=1}^n P(A_j), P\left(\bigcup_{j=1}^\infty A_j\right)=\sum_{j=1}^\infty P(A_j)
$$

> 不满足公理化条件的 $$P$$ 不是概率

<font color=green>定理 1.3.1</font> 概率 $$P$$ 有以下的性质
1. $$P(\bar A) = 1-P(A)\le 1$$
2. 单调性: $$B\subset A\Rightarrow P(B)\le P(A)$$
3. 次可加性: 对于事件 $$A_1, A_2, \cdots$$, 有
$$
P\left(\bigcup_{j=1}^n A_j\right)\le\sum_{j=1}^n P(A_j), P\left(\bigcup_{j=1}^\infty A_j\right)\le\sum_{j=1}^\infty P(A_j)
$$

如果用 $$\mathcal F\subset\mathscr P(\Omega)$$ 表示样本空间 $$\Omega$$ 的事件的全体, $$P$$ 表示概率, 则 $$(\Omega,\mathcal F, P)$$ 为概率空间.

古典概型的两个条件往往不能满足, 此时如何定义概率?常用的一种方法是把含有事件 $$A$$ 的随机试验独立重复做 $$N$$ 次 (Bernoulli 试验), 称

$$
f_N=\frac{N \text{次试验中} A \text{发生的次数}}{N}
$$

是 $$N$$ 次独立重复试验中, 事件 $$A$$ 发生的**频率**. 当 $$N$$ 越来越大时, 频率会在某个值 $$P(A)$$ 附近波动, 且波动越来越小, 这个值 $$P(A)$$ 就定义为事件A的概率.

## 几何概型
- 古典概型: 试验结果有限, 等可能性
- 几何概型: 取消 "试验结果有限", 对"等可能性" 作不同假设: 等长度 (面积, 体积), 等概率

## 加法公式

$$
P(A\cup B)=P(A)+P(B)-P(AB)
$$

$$
P\left(\bigcup_{j=1}^3A_j\right)=\sum^3_{j=1}P(A_j)-\sum_{1\le i< j\le3}P(A_iA_j)+P(A_1A_2A_3)
$$

## 事件的独立性
<font color = yellow>定义 1.6.1</font> 如果 $$P(AB)=P(A)P(B)$$, 则称 $$A,B$$ 相互独立, 简称为**独立**.

<font color=green>定理 1.6.1</font> $$A,B$$ 独立当且仅当 $$\bar A,B$$ 独立.

<font color = yellow>定义 1.6.2</font> 
1. 称 $$A_1, A_2, \cdots, A_n$$ 相互独立, 如果对任何 $$1\le j_1<j_2<\cdots<j_k\le n$$, 有 $$P\left(\bigcap_{i=1}^{k}A_{j_i}\right)=\prod_{i=1}^{k}P(A_{j_i})$$
2. 称 $$A_1, A_2, \cdots$$ 相互独立, 如果对任意的 $$n\ge2$$, 事件 $$A_1, A_2, \cdots, A_n$$ 相互独立. 这时也称 $$\{A_n\}$$ 为**独立事件列**.

> $$A,B,C$$ 两两独立不能保证它们相互独立

<font color=green>定理 1.6.2</font> 设 $$A_1, A_2, \cdots, A_n$$ 相互独立, 则
1. 对任何 $$1\le j_1<j_2<\cdots<j_k\le n$$, $$A_{j_1}, A_{j_2},\cdots, A_{j_k}$$ 相互独立.
2. 用 $$B_j$$ 表示 $$A_j$$ 或 $$\bar A_j$$, 则 $$B_1, B_2, \cdots, B_n$$ 相互独立
3. $$(A_1 A_2), A_3, \cdots, A_n$$ 相互独立
4. $$(A_1\cup A_2), A_3, \cdots, A_n$$ 相互独立

## 条件概率与乘法公式
### 条件概率
设 $$A,B$$ 是事件, 用 $$P(B\vert A)$$ 表示已知 $$A$$ 发生的条件下, $$B$$ 发生的条件概率, 简称为**条件概率**, 计算公式如下:

$$
\text{if } P(A)>0,P(B\vert A)=\frac{P(AB)}{P(A)} 
$$

### 乘法公式
设 $$P(A)>0, P\left(\bigcap^{n-1}_{j=1}A_j\right)>0$$, 则
1. $$P(AB)=P(A)P(B\vert A)$$
2. $$P\left(\bigcap^{n}_{j=1}A_j\right)=P(A_1)\prod_{i=1}^{n-1}P\left(A_{i+1}\left\vert\bigcap^{i}_{j=1}A_j\right.\right)$$

> 若 $$A,B$$ 独立, 则 $$P(B\vert A)=P(B)$$

## 全概率公式与贝叶斯公式
### 全概率公式
对于任何事件 $$A,B$$, 利用概率的可加性, 有 $$P(B)=P(AB+\bar AB)=P(AB)+P(\bar AB)$$, 再利用乘法公式得到**全概率公式**:

$$
P(B)=P(A)P(B\vert A)+P(\bar A)P(B\vert\bar A)
$$

稍做推广, 有:

<font color=green>全概率公式</font> 如果事件 $$A_1, A_2, \cdots, A_n$$ 互不相容, $$B\subset\bigcup^n_{j=1}A_j$$, 则

$$
P(B)=\sum_{j=1}^n P(A_j)P(B\vert A_j)
$$

> 如果事件 $$A_1, A_2, \cdots, A_n$$ 互不相容且 $$\bigcup^n_{j=1}A_j=\Omega$$, 则称 $$A_1, A_2, \cdots, A_n$$ 是**完备事件组**.

### 贝叶斯 (Bayes) 公式
对于事件 $$A,B$$, 当 $$P(B)>0$$, 利用条件概率公式有

$$
P(A\vert B)=\frac{P(AB)}{P(B)}=\frac{P(A)P(B\vert A)}{P(B)}
$$

对于 $$P(B)$$ 再利用全概率公式, 得到**贝叶斯公式**:

$$
P(A\vert B)=\frac{P(A)P(B\vert A)}{P(A)P(B\vert A)+P(\bar A)P(B\vert\bar A)}
$$

稍做推广, 有:

<font color=green>贝叶斯公式</font> 如果事件 $$A_1, A_2, \cdots, A_n$$ 互不相容, $$B\subset\bigcup^n_{j=1}A_j$$, 则

$$
P(A_j\vert B)=\frac{P(A_j)P(B\vert A_j)}{\sum_{i=1}^nP(A_i)P(B\vert A_i)}
$$

---

# 随机变量及概率分布

---

# 随机变量的数字特征

---

## 随机变量的数学期望
<font color = yellow>定义 3.1.1</font> 设 $$X$$ 有概率分布 $$p_j=P(X=x_j), j = 0, 1, \cdots$$, 如果有 $$\displaystyle \sum^{\infty}_{j=0}|x_j|p_j<\infty$$, 则称 $$X$$ 的数学期望存在, 并且称 $$\displaystyle E(X) = \sum^{\infty}_{j=0}|x_j|p_j $$ 为 $$X$$ 或分布 $$\{p_j\}$$ 的**数学期望**.

<font color = yellow>定义 3.1.2</font> 设 $$X$$ 有概率密度 $$f(x)$$, 如果有 $$\displaystyle \int^\infty_{-\infty}xf(x)\mathrm{d}x<\infty$$, 则称 $$X$$ 的数学期望存在, 并且称 $$\displaystyle E(X) = \int^\infty_{-\infty}xf(x)\mathrm{d}x$$ 为 $$X$$ 或 $$f(x)$$ 的**数学期望**.

### 常用的数学期望
<font color=green>定理 3.1.1</font> 设 $$X$$ 的数学期望有限, 概率密度 $$f(x)$$ 关于 $$c$$ 对称 $$\Leftrightarrow\ f(c+x)=f(c-x)$$, 则 $$E(X) = c$$.

- 伯努利分布 $$B(1,p)$$ 与二项分布 $$B(n,p)$$

$$
X\sim B(1,p) \Rightarrow E(X)=p\quad X\sim B(n,p) \Rightarrow E(X)=np
$$

-  泊松分布 $$P(\lambda)$$

$$
X\sim P(\lambda) \Rightarrow E(X)=\lambda
$$

-  几何分布 $$P(X=j)=p(1-p)^{j-1}$$

$$
E(X)=\frac{1}{p}
$$

-  指数分布 $$\mathrm{Exp}(\lambda)$$

$$
E(X)=\frac{1}{\lambda}
$$

-  均匀分布 $$U(a,b)$$

$$
E(X)=\frac{a+b}{2}
$$

-  正态分布 $$N(\mu,\sigma^2)$$

$$
E(X)=μ
$$

### 数学期望的计算
<font color=green>定理 3.1.2</font> 设 $$X,Y$$ 为离散型随机变量, $$E(g(X)), E(h(X,Y))$$ 存在.
1. 若 $$X$$ 有离散分布 $$p_j=P(X=x_j), j\ge 1$$, 则 $$\displaystyle E(g(X)) = \sum^{\infty}_{j=1}g(x_j)p_j$$.
2. 若 $$X,Y$$ 有离散分布 $$p_{ij}=P(X=x_j, Y=y_j), i,j\ge 1$$, 则$$E(h(X,Y)) = \sum^{\infty}_{i,j=1}h(x_i,y_j)p_{ij} $$

> <font color = red>例 3.1.1</font> 
> $$\displaystyle X\sim B(n,p), E[X(X-1)] = \sum^n_{j=0}j(j-1)\mathrm{C}^j_np^j(1-p)^{n-j}=p^2\left.\left(\frac{\mathrm{d}^2}{\mathrm dx^2}\sum^n_{j=0}\mathrm{C}^j_nx^j(1-p)^{n-j}\right)\right|_{x=p}=p^2\left.\frac{\mathrm{d}^2}{\mathrm dx^2}(x+q)^n\right|_{x=p}=n(n-1)p^2$$

<font color=green>定理 3.1.3</font> 设 $$X,Y$$ 为连续型随机变量, $$E(g(X)), E(h(X,Y))$$ 存在.
1. 若 $$X$$ 有概率密度 $$f(x)$$, 则 $$\displaystyle E(g(X)) = \int_{-\infty}^\infty g(x)f(x)\mathrm{d}x$$.
2. 若 $$X,Y$$ 有概率密度 $$f(x,y)$$, 则 $$\displaystyle E(h(X,Y)) = \iint_{\mathbb{R}^2} h(x,y)f(x,y)\mathrm{d}x\mathrm{d}y$$.
3. 若 $$X$$ 是非负随机变量, 则 $$\displaystyle E(X) = \int_{0}^\infty P(X>x)\mathrm{d}x$$.

> <font color=red>例 3.1.2</font> $$\displaystyle X\sim U(0,\frac{\pi}{2}), E(\cos X) = \int_0^{\pi/2}\frac{2}{\pi}\cos x\mathrm dx=\frac{2}{\pi}$$

### 数学期望的性质
<font color=green>定理 3.1.4</font> 设 $$E(X_j)<\infty (1\le j\le n)$$, $$c_0, c_1, \cdots c_n$$ 是常数, 则有
1. 线性组合 $$Y = c_0 + c_1X_1 + c_2X_2 +\cdots + c_nX_n$$ 的数学期望存在, 而且有 $$E(Y) = c_0 + \sum^n_{j=1}c_jE(X_j)$$
2. 如果 $$X_1, X_2, \cdots, X_n$$ 相互独立, 则乘积 $$Z = X_1X_2\cdots X_n$$ 的数学期望存在, 而且有 $$E(Z) = \prod_{j=1}^nE(X_j)$$
3. 如果 $$P(X_1\le X_2) = 1$$, 则 $$E(X_1)\le E(X_2)$$

<font color=green>定理 3.1.5</font> $$E(|X|)=0$$ 的充分必要条件是 $$P(X=0)=1$$
此时我们称 $$X=0$$ **以概率 1 发生**, 记作 $$X=0\ a.s.$$ . 以概率 1 发生又被称作几乎处处或几乎必然 (almost surely) 发生.

## 随机变量的方差
<font color = yellow>定义 3.2.1</font> 设 $$\mu=E(X)$$, 如果 $$E\left((X-\mu)^2\right)<\infty$$ , 则称 $$\sigma^2=E\left((X-\mu)^2\right)$$ 为 $$X$$ 的**方差**, 记作 $$Var(X)$$ 或 $$\sigma_X^2$$. 称 $$\sigma_X=\sqrt{Var(X)}$$ 为 $$X$$ 的**标准差**.
- 当 $$X$$ 有离散分布 $$p_j=P(X=x_j), j=1,2,\cdots$$ 时, 有

$$
Var(X) = E\left((X-\mu)^2\right)=\sum^\infty_{j=1}(x_j-\mu)^2p_j
$$

- 当 $$X$$ 有概率密度 $$f(x)$$ 时, 有

$$
Var(X) = E\left((X-\mu)^2\right)=\int^\infty_{-\infty}(x-\mu)^2f(x)\mathrm dx
$$

说明随机变量 $$X$$ 的方差由 $$X$$ 的概率分布唯一决定, 即

<font color = green>定理 3.2.1</font> 如果 $$X,Y$$ 有相同的概率分布, 则它们有相同的数学期望和方差

> $$X$$ 的方差描述了 $$X$$ 的分散程度, $$Var(X)$$ 越小, 说明 $$X$$ 在数学期望附近越集中. 特别地, $$Var(X) = 0\Leftrightarrow X=\mu\ a.s.$$ 

利用方差的定义有

$$
Var(X)=E\left((X-\mu)^2\right)=E\left(X^2-2X\mu+\mu^2\right)=E\left(X^2\right)-E^2(X)
$$

这就是计算方差的常用公式.

### 常用的方差
- 伯努利分布 $$B(1,p)$$ 与二项分布 $$B(n,p)$$

$$
\begin{eqnarray}
&E(X) = np, E\left(X(X-1)\right)=n(n-1)p^2\Rightarrow E\left(X^2\right)=n(n-1)p^2+np\\
&Var(X)=n(n-1)p^2+np-(np)^2=np(1-p)
\end{eqnarray}
$$

- 泊松分布 $$P(\lambda)$$

$$
\begin{aligned}
E\left(X^2\right)&=E\left(X(X-1)\right)+E(X)\\
&=\sum_{k=0}^\infty k(k-1)\frac{\lambda^k}{k!}\exp(-\lambda)+\lambda\\
&=\lambda^2\sum_{k=2}^\infty \frac{\lambda^{k-2}}{(k-2)!}\exp(-\lambda)+\lambda\\
&=\lambda^2+\lambda
\end{aligned}
\Rightarrow Var(X)=\lambda^2+\lambda-\lambda^2
$$

- 几何分布 $$P(X=j)=p(1-p)^{j-1}$$

$$
\begin{eqnarray}
&\begin{aligned}
E\left(X^2\right)&=E\left(X(X-1)\right)+E(X)\\
&=\sum_{k=0}^\infty k(k-1)p(1-p)^{k-1}+\frac{1}{p}\\
&=p(1-p)\frac{\mathrm d^2}{\mathrm d(1-p)^2}\left(\sum_{k=0}^\infty(1-p)^k\right)+\frac{1}{p}\\
&=p(1-p)\frac{\mathrm d^2}{\mathrm d(1-p)^2}\left(\frac{1}{1-(1-p)}\right)+\frac{1}{p}\\
&=\frac{2p(1-p)}{p^3}+\frac{1}{p}\\
&=\frac{2(1-p)}{p^2}+\frac{1}{p}
\end{aligned}\\
&\Rightarrow Var(X)=\frac{2(1-p)}{p^2}+\frac{1}{p}-\frac{1}{p^2}=\frac{1-p}{p^2}
\end{eqnarray}
$$

- 指数分布 $$\mathrm{Exp}(\lambda)$$

$$
\begin{aligned}
E\left(X^2\right)&=\int^\infty_0x^2\lambda \exp(-\lambda x)\mathrm dx\\
&= \frac{1}{\lambda^2}\int^\infty_0t^2\exp(-t)\mathrm dt\\
&= \frac{1}{\lambda^2}\Gamma(3)\\
&= \frac{2}{\lambda^2}
\end{aligned}
\Rightarrow Var(X)=\frac{2}{\lambda^2}-\frac{1}{\lambda^2}=\frac{1}{\lambda^2}
$$

- 均匀分布 $$U(a,b)$$

$$
E\left(X^2\right)=\int_a^b\frac{x^2}{b-a}\mathrm dx=\frac{b^3-a^3}{3(b-a)}\Rightarrow Var(x)=\frac{b^3-a^3}{3(b-1)}-\left(\frac{a+b}{12}\right)^2=\frac{(b-a)^2}{12}
$$

- 正态分布 $$N(\mu,\sigma^2)$$

$$
Var(X)=\sigma^2
$$

### 方差的性质
<font color=green>定理 3.2.2</font> 设 $$a,b,c$$ 是常数, $$E(X)=\mu, Var(X)<\infty$$,  $$E(X_j)=\mu_j, Var(X_j)<\infty (1\le j\le n)$$, 则有
1. $$Var(a+bX)=b^2Var(X)$$
2. $$\forall c\neq \mu, Var(X)=E\left((X-\mu)^2\right) <E\left((X-c)^2\right)$$ 
3. $$Var(X)=0\Leftrightarrow P(X=\mu)=1$$
4. 当 $$X_1, X_2, \cdots, X_n$$ 相互独立时, $$Var\left(\sum_{j=1}^nX_j\right)=\sum_{j=1}^nVar(X_j)$$

设 $$X\sim N(\mu,\sigma^2), Y=\frac{X-\mu}{\sigma}$$, 则 $$E(Y)=0, Var(Y)=1$$, 这时称 $$Y$$ 为 $$X$$ 的**标准化**.

## 协方差和相关系数
### 内积不等式
<font color=green>定理 3.3.1</font> 设 $$E\left(X^2\right)<\infty,E\left(Y^2\right)<\infty$$, 则有 $$|E(XY)|\le \sqrt{E\left(X^2\right)E\left(Y^2\right)}$$, 其中等号成立的充分必要条件是有不全为零的常数 $$a,b$$ 使得 $$P(aX+bY=0)=1$$
### 协方差和相关系数
<font color = yellow>定义 3.3.1</font> 设 $$\mu_X=E(X), \mu_Y=E(Y), \sigma_X=\sqrt{Var(X)}, \sigma_Y=\sqrt{Var(Y)}$$.
1. 当 $$\sigma_X,\sigma_Y$$ 存在时, 称 $$E((X-\mu_X)(Y-\mu_Y))$$ 为随机变量 $$X,Y$$ 的**协方差**, 记作 $$Cov(X,Y)$$, 或 $$\sigma_{XY}$$. 当 $$Cov(X,Y)=0$$ 时, 称 $$X,Y$$ **不相关**.
2. 当 $$0<\sigma_X\sigma_Y<\infty$$ 时, 称 $$\rho_{XY}=\frac{\sigma_{XY}}{\sigma_X\sigma_Y}$$ 为 $$X,Y$$ 的**相关系数**, $$\rho_{XY}$$ 也常用 $$\rho(X,Y)$$ 表示.

类似计算方差的常用公式, 我们有以下计算协方差的常用公式:

$$
Cov(X,Y)=E(XY)-E(X)E(Y)
$$

利用内积不等式和相关系数的定义, 我们有以下性质

<font color=green>定理 3.3.2</font> 设 $$\rho_{XY}$$ 是 $$X,Y$$ 的相关系数, 则有
1. $$|\rho_{XY}|\le 1$$
2. $$|\rho_{XY}|= 1\Leftrightarrow \exists a,b, s.t.\ P(Y=a+bX)=1$$
3. 如果 $$X,Y$$ 独立, 则 $$X,Y$$ 不相关.

- 可以看出, $$\lvert\rho_{XY}\rvert= 1$$ 时, $$X,Y$$ 有线性关系, 这时称 $$X,Y$$ **线性相关**.
- 相关系数只表示了随机变量之间的线性关系, 相关系数为零时两随机变量之间也可以有其他非线性关系, 比如 $$X\sim N(0,1), Cov\left(X,X^2\right)=E\left(X^3\right)-E(X)=0\Rightarrow\rho\left(X,X^2\right)=0$$.

### 协方差矩阵
<font color = yellow>定义 3.3.1</font> 称随机向量 $$(X_1,X_2)$$ 的协方差 $$\sigma_{ij}=Cov(X_i,X_j)$$ 构成的矩阵

$$
\Sigma=\left(
\begin{matrix}
\sigma_{11}&\sigma_{12}\\
\sigma_{21}&\sigma_{22}
\end{matrix}
\right)
$$

为 $$X$$ 的**协方差矩阵**. 因为 $$\sigma_{12}=\sigma_{21}$$, 所以协方差矩阵 $$\Sigma$$ 是对称矩阵.

<font color=green>定理 3.3.3</font> 设 $$\boldsymbol X=(X_1, X_2)$$ 有协方差矩阵 $$\Sigma$$, $$E(\boldsymbol X)=(\mu_1,\mu_2)$$, 则
1. $$\Sigma$$ 是正半定矩阵
2. $$\Sigma$$ 退化的充分必要条件是有不全为零的常数 $$a_1, a_2$$ 使得

$$
P\left(\sum_{i=1}^2a_i(X_i-\mu_i)=0\right)=1
$$

## 正态分布的参数计算
设 $$Y_1, Y_2\sim N(0,1)$$ 独立, $$ac-bd\neq0$$ 且

$$
\begin{cases}
X_1=aY_1+bY_2+\mu_1\\
X_2=cY_1+dY_2+\mu_2
\end{cases}
$$

则 
$$
Cov(X_1,X_2)=ac+bd,(X_1,X_2)\sim N(\mu_1, \mu_2;\sigma_1^2, \sigma_2^2; \rho)
$$

其中有

$$
\sigma_1^2=a^2+b^2, \sigma_2^2=c^2+d^2, \rho=\frac{ac+bd}{\sigma_1\sigma_2}
$$

容易计算得到 $$X_1, X_2$$ 的数字特征

$$
\begin{cases}
E(X_1)=\mu_1 & E(X_2) = \mu_2\\
Var(X_1)=\sigma_1^2 & Var(X_2)= \sigma_2^2\\
\rho_{X_1X_2}=\rho
\end{cases}
$$

<font color=green>定理 3.4.1</font> 如果 $$(X_1,X_2)\sim N(\mu_1, \mu_2;\sigma_1^2, \sigma_2^2; \rho)$$, 则 $$X_1, X_2$$ 的数字特征如上式, 且 $$X_1, X_2$$ 独立的充分必要条件是 $$X_1, X_2$$ 不相关

---

# 大数定律和中心极限定理

## 强大数律

<font color = yellow>定义 4.1.1</font> 我们用 $$A\ a.s.$$ 表示 $$P(A)=1$$, 即

$$
P(A)=1\Leftrightarrow A\ a.s.
$$

<font color=green>定理 4.1.1</font> (强大数律) 如果 $$X_1,X_2. \cdots$$ 是独立同分布的随机变量, $$\mu=E(X_1)$$, 则

$$
\overline{X_n}=\frac{1}{n}\sum^n_{j=1}X_j, \lim_{n\to\infty}\overline{X_n}=\mu\ a.s.
$$

因为概率等于 1 的事件在实际中必然发生, 所以在强大数律中, 如果用 $$x_n$$ 表示 $$X_n$$ 的观测值, 则有

$$
\lim_{n\to\infty}\frac{x_1+x_2+\cdots+x_n}{n}=\mu
$$

> 因为强大数律的数学证明不需要概率的频率定义, 所以它从理论上保证了概率的频率定义是正确的.

## 弱大数律

有强大数律, 自然就有弱大数律. 为了介绍弱大数律, 先介绍随机变量的**依概率收敛**和**切比雪夫不等式**.

<font color = yellow>定义 4.2.1</font> 设 $$U, U_1, U_2, \cdots$$ 是随机变量. 若 $$\forall\varepsilon>0, \lim_{n\to\infty}P(\lvert U_n-U\rvert\ge\varepsilon)=0$$, 则称 $$U_n$$ 依概率收敛到 $$U$$, 记作 $$U_n\xrightarrow{P}U$$.

<font color=green>引理 4.2.1</font> (**切比雪夫不等式**) 设随机变量 $$X$$ 有数学期望 $$\mu$$ 和方差 $$\sigma^2$$, 则对常数 $$\varepsilon>0$$, 有

$$
P(\lvert X-\mu\rvert\ge\varepsilon)\le\frac{\sigma^2}{\varepsilon^2}
$$

---

**证明:** 记 $$I[A]$$ 为事件 $$A$$ 的示性函数, $$Y=\lvert X-\mu\rvert$$, 则无论 $$\{Y\ge\varepsilon\}$$ 是否发生, 总有

$$
I[\{Y\ge\varepsilon\}]\le\frac{Y^2}{\varepsilon^2}
$$

因此

$$
\begin{aligned}
P(\lvert X-\mu\rvert\ge\varepsilon)&=P(Y\ge\varepsilon)=E(I[\{Y\ge\varepsilon\}])\\
&\le E\left(\frac{Y^2}{\varepsilon^2}\right)=\frac{E(Y^2)}{\varepsilon^2}=\frac{\sigma^2}{\varepsilon^2}
\end{aligned}
$$

---

> 切比雪夫不等式是概率论中最重要和最基本的不等式.

<font color=green>推论 4.2.2</font> (**弱大数律**) 设随机变量 $$X_1, X_2, \cdots$$ 独立同分布, $$\mu=E(X_1)$$, 则对任何 $$\varepsilon>0$$, 有

$$
\lim_{n\to\infty} P\left(\lvert\overline{X_n}-\mu\rvert\ge\varepsilon\right)=0
$$

## 中心极限定理

> 强大数律和弱大数律分别讨论了随机变量的样本均值的几乎处处收敛和依概率收敛. 中心极限定律研究当 $$n$$ 比较大时, 随机变量的部分和 $$S_n=\sum^n_{j=1}X_j$$ 的概率分布问题.

独立同分布随机变量和的分布近似于正态分布.

<font color=green>定理 4.3.1</font> (**中心极限定理**) 设随机变量 $$X_1, X_2, \cdots$$ 独立同分布, $$E(X_1)=\mu, Var(X_1)=\sigma^2>0$$. 记 $$S_n=\sum^n_{j=1}X_j$$ 为部分和, $$Z_n=\frac{S_n-n\mu}{\sqrt{n\sigma^2}}$$ 为 $$S_n$$ 的标准化, $$\Phi(x)$$ 表示服从 $$N(0,1)$$ 的分布函数. 当 $$n\to\infty$$ 时, 有

$$
P(Z_n\le x)\to\Phi(x), x\in\mathbb R
$$

称 $$Z_n$$ 依概率收敛到 $$N(0,1)$$, 记作 $$Z_n\xrightarrow d N(0,1)$$.

注意, 样本均值 $$\overline{X_n}$$ 的标准化等于部分和 $$S_n$$ 的标准化:

$$
Z_n=\frac{S_n-n\mu}{\sqrt{n\sigma^2}}=\frac{\overline{X_n}-\mu}{\sqrt{\sigma^2/n}}
$$

<font color=green>推论 4.3.2</font> 在定理 4.3.1 的条件下, 对较大的 $$n$$, 有

$$
\begin{eqnarray}
P\left(\frac{S_n-n\mu}{\sqrt{n\sigma^2}}\le x\right)\approx\Phi(x)\\
P\left(\frac{\overline{X_n}-\mu}{\sqrt{\sigma^2/n}}\le x\right)\approx\Phi(x)
\end{eqnarray}
$$

其中不知道方差时, 可以用样本方差

$$
\hat\sigma^2=\frac{1}{n-1}\sum^n_{j=1}\left(X_j-\overline{X_n}\right)^2\quad\mathrm{或}\quad\hat\sigma^2=\frac{1}{n}\sum^n_{j=1}\left(X_j-\overline{X_n}\right)^2
$$

代替 $$\sigma^2$$.

